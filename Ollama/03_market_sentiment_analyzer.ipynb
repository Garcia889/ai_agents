{"cells":[{"cell_type":"markdown","id":"1cb441b5","metadata":{"cell_marker":"'''","id":"1cb441b5"},"source":["Refactoring for Ollama Integration\n","=======================================================================\n","\n","The goal of this excercise is to refactor the implementations from the prompting excercises to integrate Ollama while retaining the existing OpenAI API calls. This exercise will allow us to compare responses between OpenAI and a local LLM running on Ollama, helping us evaluate the feasibility of using local models and gain hands-on experience working with them.\n"]},{"cell_type":"markdown","id":"3878d1bb","metadata":{"cell_marker":"'''","id":"3878d1bb"},"source":["## Step 0: Setup and Dependencies\n","--------------------------------\n","First, let's ensure we have all required packages installed."]},{"cell_type":"code","execution_count":2,"id":"6374d2c6","metadata":{"id":"6374d2c6"},"outputs":[],"source":["!pip install numpy pandas matplotlib langchain openai python-dotenv typing-extensions pydantic pydantic_settings langchain-community langchain-openai --quiet"]},{"cell_type":"code","execution_count":null,"id":"885a171c","metadata":{},"outputs":[],"source":["# Install Ollama\n","!pip install ollama"]},{"cell_type":"markdown","id":"5f3a2b92","metadata":{"cell_marker":"'''","id":"5f3a2b92"},"source":["## Step 1: Initial Configuration\n","--------------------------------\n","Set up our environment and imports."]},{"cell_type":"code","execution_count":38,"id":"6a4f8ad9","metadata":{"id":"6a4f8ad9"},"outputs":[],"source":["from typing import Any, Dict, List\n","from langchain.chat_models import ChatOpenAI\n","import ollama\n","import requests"]},{"cell_type":"markdown","id":"f58f2fa0","metadata":{},"source":["#### The next class is added to send prompts to our Ollama LLM running in localhost:"]},{"cell_type":"code","execution_count":39,"id":"29881813","metadata":{},"outputs":[],"source":["class OllamaClient:\n","    \"\"\"\n","    A client for interacting with the Ollama API to generate responses based on a given prompt.\n","\n","    Attributes:\n","        model (str): The model to use for generating responses. Default is \"llama3.2\".\n","        stream (bool): Flag to determine whether to use streaming for responses. Default is False.\n","\n","    Methods:\n","        __call__(prompt):\n","            Sends a prompt to the Ollama API and returns the complete response.\n","            If streaming is enabled, concatenates the response lines.\n","        \n","            Args:\n","                prompt (str): The input prompt to send to the Ollama API.\n","            \n","            Returns:\n","                str: The complete response from the Ollama API.\n","    \"\"\"\n","    def __init__(self, model=\"llama3.2\", stream=False):\n","        self.model = model\n","        self.stream = stream  # Allows choosing whether to use streaming or not\n","\n","    def __call__(self, prompt):\n","        \"\"\"Sends a prompt to Ollama and returns the complete response\"\"\"\n","        response = requests.post(\n","            \"http://localhost:11434/api/generate\",\n","            json={\"model\": self.model, \"prompt\": prompt, \"stream\": self.stream}\n","        )\n","\n","        if not self.stream:  # If streaming is disabled, return the response as is\n","            return response.json().get(\"response\", \"\")\n","\n","        # If streaming is enabled, concatenate the response lines\n","        full_response = \"\"\n","        for line in response.iter_lines():\n","            if line:\n","                try:\n","                    data = requests.utils.json.loads(line)\n","                    full_response += data.get(\"response\", \"\")\n","                except Exception as e:\n","                    print(\"\\nError al procesar una lÃ­nea:\", e)\n","\n","        return full_response\n"]},{"cell_type":"markdown","id":"c69ecffd","metadata":{"cell_marker":"'''","id":"c69ecffd"},"source":["## Step 1.5: Configuration Management\n","--------------------------------\n","Set up configuration management for Ollama. In this part we set our LLM in a similar way as we did with OpenAI in the prompting exercises"]},{"cell_type":"code","execution_count":40,"id":"f15db154","metadata":{"id":"f15db154"},"outputs":[],"source":["import os\n","from typing import Optional\n","from pydantic_settings import BaseSettings\n","from langchain_openai import ChatOpenAI\n","from dotenv import load_dotenv\n","\n","class Settings(BaseSettings):\n","    \"\"\"\n","    Settings class to manage configuration variables for the application.\n","\n","    This class loads environment variables from a .env file and provides\n","    access to these variables as class attributes.\n","\n","    Attributes:\n","        ollama_model_name (str): The name of the Ollama model, loaded from the\n","                                 environment variable 'OLLAMA_MODEL_NAME'.\n","    \"\"\"\n","\n","    # Load variables from .env\n","    load_dotenv()\n","\n","    ollama_model_name: str = os.getenv('OLLAMA_MODEL_NAME')"]},{"cell_type":"code","execution_count":41,"id":"3b559b85","metadata":{"id":"3b559b85","lines_to_next_cell":1},"outputs":[],"source":["def setup_environment() -> OllamaClient:\n","    \"\"\"Initialize environment and create LLM instance.\n","\n","    This function:\n","    1. Loads settings\n","    2. Sets environment variables\n","    3. Initializes chat model\n","\n","    Returns:\n","        Ollama: Configured language model instance\n","    \"\"\"\n","    # Load settings\n","    settings = Settings()\n","\n","    # Initialize ChatOpenAI with settings\n","    llm = OllamaClient(\n","        model=settings.ollama_model_name\n","    )\n","\n","    return llm"]},{"cell_type":"code","execution_count":42,"id":"1a79a13e","metadata":{"id":"1a79a13e"},"outputs":[],"source":["# Initialize LLM\n","try:\n","    llm = setup_environment()\n","except Exception as e:\n","    print(f\"Error initializing LLM: {e}\")"]},{"cell_type":"code","execution_count":46,"id":"87a0e30d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Response: I am a text-based conversational AI model, specifically a large language model (LLM). My architecture is based on transformer models, which use self-attention mechanisms to process and generate human-like text.\n","\n","My exact model is a variant of the BART (Bidirectional Architecture for Task-oriented Response generation) model, which is a popular LLM designed for natural language processing tasks such as text generation, question answering, and conversation.\n","\n","I was trained on a massive dataset of text from various sources, including books, articles, and online content. This training allows me to generate human-like responses to a wide range of questions and topics.\n","\n","However, I'd like to clarify that I'm not a single, specific model, but rather a general term for a type of LLM designed for conversational AI tasks. There are many other LLMs out there, each with their own strengths and weaknesses, and I'm constantly learning and improving my abilities based on the interactions I have with users like you!\n","Execution Time: 35.08425498008728 seconds\n"]}],"source":["import time\n","\n","# Test LLM with a prompt\n","start_time = time.time()\n","response = llm(f\"What LLM are you?\")\n","end_time = time.time()\n","\n","print(\"Response:\", response)\n","print(\"Execution Time:\", end_time - start_time, \"seconds\")\n"]},{"cell_type":"markdown","id":"0de6fa25","metadata":{"cell_marker":"'''","id":"0de6fa25"},"source":["## Problem: Market Sentiment Analysis System\n","--------------------------------\n","Design and implement a comprehensive market sentiment analysis system\n","that combines multiple data sources and ensures consistency.\n","\n","### Requirements:\n","1. Multi-Source Analysis:\n","   - News articles and headlines\n","   - Social media sentiment\n","   - Technical indicators\n","   - Market statistics\n","   - Analyst reports\n","\n","2. Self-Consistency Checks:\n","   - Cross-validation of sources\n","   - Internal consistency metrics\n","   - Temporal consistency\n","   - Source reliability scoring\n","\n","3. Confidence Scoring:\n","   - Source-specific confidence\n","   - Analysis reliability metrics\n","   - Consensus confidence\n","   - Time-sensitivity factors"]},{"cell_type":"code","execution_count":47,"id":"3d6bf733","metadata":{},"outputs":[],"source":["#Ollama\n","from typing import Dict, List, Any\n","from collections import defaultdict\n","import numpy as np\n","\n","class MarketSentimentAnalyzer:\n","    \"\"\"A system for comprehensive market sentiment analysis.\"\"\"\n","\n","    def __init__(self, llm: Any):\n","        \"\"\"Initialize sentiment analysis system.\"\"\"\n","        self.llm = llm\n","        self.source_weights = {\n","            \"news_articles\": 0.3,\n","            \"social_media_sentiment\": 0.25,\n","            \"technical_indicators\": 0.2,\n","            \"market_statistics\": 0.15,\n","            \"analyst_reports\": 0.1,\n","        }\n","        self.source_reliability = defaultdict(lambda: 0.8)  # Default reliability score\n","        self.temporal_decay_factor = 0.95  # Decay factor for older data\n","\n","    def analyze_sentiment(self, market_data: Dict[str, str]) -> Dict[str, Any]:\n","        \"\"\"Analyze market sentiment from multiple sources.\"\"\"\n","        analyses = {}\n","        for source, content in market_data.items():\n","            if source == \"news_articles\":\n","                analyses[source] = self._analyze_news(content)\n","            elif source == \"social_media_sentiment\":\n","                analyses[source] = self._analyze_social_media(content)\n","            elif source == \"technical_indicators\":\n","                analyses[source] = self._analyze_technical_indicators(content)\n","            elif source == \"market_statistics\":\n","                analyses[source] = self._analyze_market_statistics(content)\n","            elif source == \"analyst_reports\":\n","                analyses[source] = self._analyze_analyst_reports(content)\n","            else:\n","                raise ValueError(f\"Unknown data source: {source}\")\n","\n","        # Assign confidence scores\n","        for source, analysis in analyses.items():\n","            analysis[\"confidence\"] = self._calculate_source_confidence(source, analysis)\n","\n","        return analyses\n","\n","    def check_consistency(self, analyses: List[Dict[str, Any]]) -> float:\n","        \"\"\"Check consistency between different analyses.\"\"\"\n","        sentiment_scores = []\n","        for analysis in analyses.values():\n","            sentiment_scores.append(analysis[\"sentiment_score\"])\n","\n","        # Calculate consistency as the inverse of standard deviation\n","        consistency = 1 / (np.std(sentiment_scores) + 1e-9)  # Avoid division by zero\n","        return min(consistency, 1.0)  # Cap consistency at 1.0\n","\n","    def generate_consensus(self, analyses: List[Dict[str, Any]]) -> Dict[str, Any]:\n","        \"\"\"Generate weighted consensus with confidence scores.\"\"\"\n","        weighted_sentiment = 0\n","        total_weight = 0\n","        for source, analysis in analyses.items():\n","            weight = self.source_weights[source] * analysis[\"confidence\"]\n","            weighted_sentiment += analysis[\"sentiment_score\"] * weight\n","            total_weight += weight\n","\n","        consensus_sentiment = weighted_sentiment / total_weight\n","        consistency_score = self.check_consistency(analyses)\n","\n","        return {\n","            \"consensus_sentiment\": consensus_sentiment,\n","            \"consistency_score\": consistency_score,\n","            \"confidence\": total_weight / sum(self.source_weights.values()),\n","        }\n","\n","    def _analyze_news(self, content: str) -> Dict[str, Any]:\n","        \"\"\"Analyze sentiment from news articles.\"\"\"\n","        # Use LLM to extract sentiment\n","        response = self.llm(f\"Analyze the sentiment of the following news articles:\\n{content}\")\n","        sentiment_score = self._extract_sentiment_score(response)  # Extract text from AIMessage\n","        return {\"sentiment_score\": sentiment_score, \"source\": \"news_articles\"}\n","\n","    def _analyze_social_media(self, content: str) -> Dict[str, Any]:\n","        \"\"\"Analyze sentiment from social media.\"\"\"\n","        # Initialize default values\n","        positive_mentions = 0.0\n","        negative_mentions = 0.0\n","\n","        # Extract positive mentions\n","        if \"positive mentions\" in content:\n","            try:\n","                positive_mentions = float(content.split(\"positive mentions\")[1].split(\"%\")[0]) / 100\n","            except (IndexError, ValueError):\n","                pass  # Use default value if parsing fails\n","\n","        # Extract negative mentions\n","        if \"negative mentions\" in content:\n","            try:\n","                negative_mentions = float(content.split(\"negative mentions\")[1].split(\"%\")[0]) / 100\n","            except (IndexError, ValueError):\n","                pass  # Use default value if parsing fails\n","\n","        # Calculate sentiment score\n","        sentiment_score = positive_mentions - negative_mentions\n","        return {\"sentiment_score\": sentiment_score, \"source\": \"social_media_sentiment\"}\n","\n","    def _analyze_technical_indicators(self, content: str) -> Dict[str, Any]:\n","        \"\"\"Analyze sentiment from technical indicators.\"\"\"\n","        # Extract RSI and VIX\n","        rsi = 50.0  # Default value\n","        vix = 20.0  # Default value\n","\n","        if \"RSI:\" in content:\n","            try:\n","                rsi = float(content.split(\"RSI:\")[1].split(\"\\n\")[0])\n","            except (IndexError, ValueError):\n","                pass  # Use default value if parsing fails\n","\n","        if \"VIX:\" in content:\n","            try:\n","                vix = float(content.split(\"VIX:\")[1].split(\"\\n\")[0])\n","            except (IndexError, ValueError):\n","                pass  # Use default value if parsing fails\n","\n","        # Calculate sentiment score\n","        sentiment_score = (rsi - 50) / 50 - (vix - 20) / 20  # Normalize and combine\n","        return {\"sentiment_score\": sentiment_score, \"source\": \"technical_indicators\"}\n","\n","    def _analyze_market_statistics(self, content: str) -> Dict[str, Any]:\n","        \"\"\"Analyze sentiment from market statistics.\"\"\"\n","        # Extract NASDAQ performance\n","        nasdaq_change = 0.0  # Default value\n","\n","        if \"NASDAQ:\" in content:\n","            try:\n","                nasdaq_change = float(content.split(\"NASDAQ:\")[1].split(\"%\")[0]) / 100\n","            except (IndexError, ValueError):\n","                pass  # Use default value if parsing fails\n","\n","        # Use NASDAQ as a proxy for tech sentiment\n","        sentiment_score = nasdaq_change\n","        return {\"sentiment_score\": sentiment_score, \"source\": \"market_statistics\"}\n","\n","    def _analyze_analyst_reports(self, content: str) -> Dict[str, Any]:\n","        \"\"\"Analyze sentiment from analyst reports.\"\"\"\n","        # Use LLM to extract sentiment\n","        response = self.llm(f\"Analyze the sentiment of the following analyst reports:\\n{content}\")\n","        sentiment_score = self._extract_sentiment_score(response)  # Extract text from AIMessage\n","        return {\"sentiment_score\": sentiment_score, \"source\": \"analyst_reports\"}\n","\n","    def _calculate_source_confidence(self, source: str, analysis: Dict[str, Any]) -> float:\n","        \"\"\"Calculate confidence score for a source.\"\"\"\n","        base_confidence = self.source_reliability[source]\n","        temporal_decay = self.temporal_decay_factor  # Adjust based on data freshness\n","        return base_confidence * temporal_decay\n","\n","    def _extract_sentiment_score(self, text: str) -> float:\n","        \"\"\"\n","        Extract a sentiment score from LLM response.\n","\n","        This function sends a text to an LLM to analyze its sentiment and returns a score between -1 and 1.\n","\n","        Parameters:\n","        - text (str): The text to analyze for sentiment.\n","\n","        Returns:\n","        - float: A sentiment score between -1 and 1.\n","\n","        Raises:\n","        - ValueError: If the input text is not a string or if the LLM response is not a valid float between -1 and 1.\n","        \"\"\"\n","        if not isinstance(text, str):\n","            raise ValueError(\"Input text must be a string.\")\n","\n","        try:\n","            response = llm(f\"Analyze the sentiment of the following text:\\n{text},\\\n","                            please provide only a score between -1 and 1\\\n","                            the score should be a float number\\\n","                            -Answer most be only a float number between -1 and 1\\\n","                            -Force the answer to be like the next example: Score=0.5\")\n","\n","            # Parse the response to extract the sentiment score\n","            score = float(response.split(\"=\")[1])     \n","\n","            if not (-1 <= score <= 1):\n","                raise ValueError(\"Sentiment score must be between -1 and 1.\")\n","\n","            return score\n","\n","        except Exception as e:\n","            raise ValueError(f\"Error in extracting sentiment score: {e}\")\n"]},{"cell_type":"markdown","id":"743c1b33","metadata":{"cell_marker":"'''","id":"743c1b33"},"source":["### Example Test Data:"]},{"cell_type":"code","execution_count":48,"id":"bf210262","metadata":{"id":"bf210262"},"outputs":[],"source":["market_data = {\n","    \"news_articles\": \"\"\"\n","HEADLINE: Tech Stocks Rally on Strong Earnings\n","(Reuters) - Technology stocks surged today following better-than-expected\n","earnings from major players. Apple Inc. and Microsoft Corp. both beat analyst\n","estimates, driving broader market gains. Cloud computing and AI segments\n","showed particular strength.\n","\n","HEADLINE: Fed Signals Potential Rate Cuts\n","The Federal Reserve indicated openness to rate cuts later this year, citing\n","moderating inflation pressures. Markets responded positively to the news,\n","with bond yields declining.\n","\n","HEADLINE: Startup Funding Shows Signs of Recovery\n","Venture capital investments increased 15% in Q4, marking the first\n","quarterly rise since 2022. Software and fintech sectors led the recovery.\n","\"\"\",\n","\n","    \"social_media_sentiment\": \"\"\"\n","$AAPL trending positive:\n","- 65% positive mentions\n","- 28% neutral mentions\n","- 7% negative mentions\n","Volume: 50,000 mentions\n","\n","$MSFT sentiment metrics:\n","- 72% positive mentions\n","- 22% neutral mentions\n","- 6% negative mentions\n","Volume: 35,000 mentions\n","\n","#TechStocks trending topics:\n","1. #EarningsSeason\n","2. #TechRally\n","3. #InvestInTech\n","\"\"\",\n","\n","    \"technical_indicators\": \"\"\"\n","Market Technical Analysis:\n","- S&P 500 RSI: 62.5\n","- NASDAQ RSI: 65.8\n","- VIX: 16.5\n","- Moving Averages: Most above 200-day\n","- Volume: +25% vs 30-day average\n","- Advance/Decline: 2.5:1\n","\"\"\",\n","\n","    \"market_statistics\": \"\"\"\n","Market Overview:\n","- S&P 500: +1.2%\n","- NASDAQ: +1.8%\n","- DOW: +0.9%\n","- Small Caps: +1.5%\n","- Sector Leaders: Tech +2.3%, Communications +1.9%\n","- Sector Laggards: Utilities -0.4%, Real Estate -0.2%\n","\"\"\",\n","\n","    \"analyst_reports\": \"\"\"\n","Goldman Sachs: Overweight Tech Sector\n","- Target price revisions: +10% average\n","- Sector outlook: Positive\n","- Key drivers: AI adoption, cloud growth\n","- Risk factors: Valuations, rate sensitivity\n","\n","Morgan Stanley: Market Outlook\n","- Stance: Constructively bullish\n","- Focus areas: Quality growth stocks\n","- Concerns: Geopolitical tensions\n","- 12-month S&P target: 5200\n","\"\"\"\n","}"]},{"cell_type":"code","execution_count":49,"id":"42682948","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Analyzer Check Time: 249.7772490978241 seconds\n","Consensus Sentiment: 0.38769999999999993\n","Consensus Check Time: 0.000308990478515625 seconds\n","Consistency Score: 1.0\n","Consistency Check Time: 0.03458714485168457 seconds\n","Confidence: 0.76\n"]}],"source":["# Initialize the analyzer with an LLM instance\n","analyzer = MarketSentimentAnalyzer(llm)\n","\n","analyzer_start_time = time.time()\n","analyses = analyzer.analyze_sentiment(market_data)\n","analyzer_end_time = time.time()\n","\n","# Check consistency\n","consistency_start_time = time.time()\n","consistency_score = analyzer.check_consistency(analyses)\n","consistency_end_time = time.time()\n","\n","# Generate consensus\n","consensus_start_time = time.time()\n","consensus = analyzer.generate_consensus(analyses)\n","consensus_end_time = time.time()\n","\n","print(\"Analyzer Check Time:\", analyzer_end_time - analyzer_start_time, \"seconds\")\n","print(\"Consensus Sentiment:\", consensus[\"consensus_sentiment\"])\n","print(\"Consensus Check Time:\", consensus_end_time - consensus_start_time, \"seconds\")\n","print(\"Consistency Score:\", consensus[\"consistency_score\"])\n","print(\"Consistency Check Time:\", consistency_end_time - consistency_start_time, \"seconds\")\n","print(\"Confidence:\", consensus[\"confidence\"])\n"]}],"metadata":{"colab":{"provenance":[]},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
